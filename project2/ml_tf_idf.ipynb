{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, nltk, pickle, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"twitter-datasets/train_neg_proc.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    neg_DF = pd.read_csv(myfile, header=None)\n",
    "with open(\"twitter-datasets/train_pos_proc.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    pos_DF = pd.read_csv(myfile, header=None)\n",
    "with open(\"twitter-datasets/test_data_proc.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    test_DF = pd.read_csv(myfile, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack difficulty object disassemble...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad dot taks tomorrow thankful startho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>v celtic regular season fucked play playoff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>could actually kill girl sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>find hard believe im afraid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wish could night tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>got kicked wgm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>yes tell lip closed okay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hi harry havea good time au didnt get see mayb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>introduction programming nd edition solid foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>introduction programming nd edition solid foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>introduction programming nd edition solid foun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>white aw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dan love miss sad wheresthegeneral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>many wonderful building dc still miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>annoying secretly find good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>post boom spanish american fiction suny series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>layer heart paperback journey inspired recent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>layer heart paperback journey inspired recent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>guess texted want u back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>farrow litter offence wilful dispersal piglet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>people barca ground tonight gonna see chelsea ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>mlb show video game mlb show</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>justinbieber following</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>justinbieber following</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>justinbieber following</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>even care dont want white as anyways</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>ugh idk got hope plan never work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>um playing trial hd ignoring fact need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246127</th>\n",
       "      <td>need go library sound like hell outside</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246128</th>\n",
       "      <td>miss lil si left alone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246129</th>\n",
       "      <td>silver alert issued elderly leesburg man orlan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246130</th>\n",
       "      <td>finished game leg hurt like crap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246131</th>\n",
       "      <td>want jeep bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246132</th>\n",
       "      <td>sad phone cannot download twitter ahh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246133</th>\n",
       "      <td>mother remember beat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246134</th>\n",
       "      <td>extremal combinatorics application computer sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246135</th>\n",
       "      <td>african presence early america paperback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246136</th>\n",
       "      <td>really sad charlotte though fallen far revenge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246137</th>\n",
       "      <td>know keeping fit difficult struggling maintain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246138</th>\n",
       "      <td>dont know yet al let know knackered stopped wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246139</th>\n",
       "      <td>wish vip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246140</th>\n",
       "      <td>sad dang lonely guess best good enough tired f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246141</th>\n",
       "      <td>oh im sure youtube</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246142</th>\n",
       "      <td>tomarrow would impossible nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246143</th>\n",
       "      <td>hell yeah phone huge hurt soo much lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246144</th>\n",
       "      <td>really wish could london jealous people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246145</th>\n",
       "      <td>know help though always like always</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246146</th>\n",
       "      <td>funny guess better zumba hotpants haha x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246147</th>\n",
       "      <td>ab sore get tryin get lean tummy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246148</th>\n",
       "      <td>market seller wah movie lool omotola role movi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246149</th>\n",
       "      <td>let go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246150</th>\n",
       "      <td>harry seriously following back never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246151</th>\n",
       "      <td>st croix avid casting rod model avc mf differe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246152</th>\n",
       "      <td>im sorry guy ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246153</th>\n",
       "      <td>find food coloring anywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246154</th>\n",
       "      <td>tort wonder chose crime module hahaa trying im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246155</th>\n",
       "      <td>keyless entry remote fob clicker buick lacross...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246156</th>\n",
       "      <td>yeap doctor know wrong lol either migraine idk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1246157 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0\n",
       "0        vinco tresorpack difficulty object disassemble...\n",
       "1                  glad dot taks tomorrow thankful startho\n",
       "2              v celtic regular season fucked play playoff\n",
       "3                           could actually kill girl sorry\n",
       "4                              find hard believe im afraid\n",
       "5                                 wish could night tonight\n",
       "6                                           got kicked wgm\n",
       "7                                 yes tell lip closed okay\n",
       "8                                                  perfect\n",
       "9        hi harry havea good time au didnt get see mayb...\n",
       "10       introduction programming nd edition solid foun...\n",
       "11       introduction programming nd edition solid foun...\n",
       "12       introduction programming nd edition solid foun...\n",
       "13                                                white aw\n",
       "14                      dan love miss sad wheresthegeneral\n",
       "15                   many wonderful building dc still miss\n",
       "16                             annoying secretly find good\n",
       "17       post boom spanish american fiction suny series...\n",
       "18       layer heart paperback journey inspired recent ...\n",
       "19       layer heart paperback journey inspired recent ...\n",
       "20                                guess texted want u back\n",
       "21       farrow litter offence wilful dispersal piglet ...\n",
       "22       people barca ground tonight gonna see chelsea ...\n",
       "23                            mlb show video game mlb show\n",
       "24                                  justinbieber following\n",
       "25                                  justinbieber following\n",
       "26                                  justinbieber following\n",
       "27                    even care dont want white as anyways\n",
       "28                        ugh idk got hope plan never work\n",
       "29                  um playing trial hd ignoring fact need\n",
       "...                                                    ...\n",
       "1246127            need go library sound like hell outside\n",
       "1246128                             miss lil si left alone\n",
       "1246129  silver alert issued elderly leesburg man orlan...\n",
       "1246130                   finished game leg hurt like crap\n",
       "1246131                                      want jeep bad\n",
       "1246132              sad phone cannot download twitter ahh\n",
       "1246133                               mother remember beat\n",
       "1246134  extremal combinatorics application computer sc...\n",
       "1246135           african presence early america paperback\n",
       "1246136     really sad charlotte though fallen far revenge\n",
       "1246137  know keeping fit difficult struggling maintain...\n",
       "1246138  dont know yet al let know knackered stopped wo...\n",
       "1246139                                           wish vip\n",
       "1246140  sad dang lonely guess best good enough tired f...\n",
       "1246141                                 oh im sure youtube\n",
       "1246142                  tomarrow would impossible nothing\n",
       "1246143             hell yeah phone huge hurt soo much lol\n",
       "1246144            really wish could london jealous people\n",
       "1246145                know help though always like always\n",
       "1246146           funny guess better zumba hotpants haha x\n",
       "1246147                   ab sore get tryin get lean tummy\n",
       "1246148  market seller wah movie lool omotola role movi...\n",
       "1246149                                             let go\n",
       "1246150               harry seriously following back never\n",
       "1246151  st croix avid casting rod model avc mf differe...\n",
       "1246152                            im sorry guy ridiculous\n",
       "1246153                        find food coloring anywhere\n",
       "1246154  tort wonder chose crime module hahaa trying im...\n",
       "1246155  keyless entry remote fob clicker buick lacross...\n",
       "1246156  yeap doctor know wrong lol either migraine idk...\n",
       "\n",
       "[1246157 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorizer. We go with the idea that we do not want the words that appear in less than 5 tweets and in more than 80% of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vectoriser\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create a corpus. Our train set would both positive and negative, and our test set is, obviously, the unlabeled part.\n",
    "\n",
    "To do this, we will append both negative and positive DF, then create a matrix of labels for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we thus know that all the first ones are labeled as -1 and all the others as 1\n",
    "all_labeled_DF = pd.concat([neg_DF, pos_DF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create the labels\n",
    "negs = len(neg_DF.index)\n",
    "poss = len(pos_DF.index)\n",
    "labels = np.zeros(negs+poss)\n",
    "labels[0:negs]=-1\n",
    "labels[negs:negs+poss]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lemmed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13892)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2392\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2393\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2394\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5169)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lemmed'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5085)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item (pandas/_libs/hashtable.c:13892)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-55ec36745c39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_corpus_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labeled_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lemmed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_corpus_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lemmed\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2060\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2062\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2064\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2067\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2069\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1533\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1534\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1535\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1536\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3589\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3590\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3591\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3592\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2393\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2395\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5239)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc (pandas/_libs/index.c:5169)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lemmed'"
     ]
    }
   ],
   "source": [
    "train_corpus_tf_idf = vectorizer.fit_transform(all_labeled_DF) \n",
    "test_corpus_tf_idf = vectorizer.transform(test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create both models\n",
    "model1 = LinearSVC() # SVM\n",
    "model2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on the given models\n",
    "model1.fit(train_corpus_tf_idf,labels)\n",
    "model2.fit(train_corpus_tf_idf,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "result1 = model1.predict(test_corpus_tf_idf)\n",
    "result2 = model2.predict(test_corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result1 and result2 are the labels predicted for the tweets we got in the test corpus. This means we probably jsute have to transforme this into a csv as it is shown in the sample submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting it to integer for prediction csv\n",
    "result1 = [int(x) for x in result1]\n",
    "result2 = [int(x) for x in result2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_df = pd.DataFrame(result1)\n",
    "svm_df['Id'] = svm_df.index + 1\n",
    "svm_df['Prediction'] = svm_df[0]\n",
    "svm_df = svm_df[['Id', 'Prediction']]\n",
    "svm_df.to_csv('svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_df = pd.DataFrame(result2)\n",
    "bayes_df['Id'] = bayes_df.index + 1\n",
    "bayes_df['Prediction'] = bayes_df[0]\n",
    "bayes_df = bayes_df[['Id', 'Prediction']]\n",
    "bayes_df.to_csv('bayes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
