{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, nltk, pickle, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('outputs/train_neg_proc.pkl', 'rb') as f:\n",
    "    neg_DF = pickle.load(f)\n",
    "    \n",
    "with open('outputs/train_pos_proc.pkl', 'rb') as f:\n",
    "    pos_DF = pickle.load(f)\n",
    "    \n",
    "with open('outputs/test_data_proc.pkl', 'rb') as f:\n",
    "    test_DF = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmed</th>\n",
       "      <th>both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[vinco, tresorpack, difficulty, object, disass...</td>\n",
       "      <td>[vinco, tresorpack, difficulti, object, disass...</td>\n",
       "      <td>[vinco, tresorpack, difficulty, object, disass...</td>\n",
       "      <td>[vinco, tresorpack, difficulti, object, disass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[glad, dot, taks, tomorrow, thankful, startho]</td>\n",
       "      <td>[glad, dot, tak, tomorrow, thank, startho]</td>\n",
       "      <td>[glad, dot, taks, tomorrow, thankful, startho]</td>\n",
       "      <td>[glad, dot, tak, tomorrow, thank, startho]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[vs, celtics, regular, season, fucked, play, p...</td>\n",
       "      <td>[vs, celtic, regular, season, fuck, play, play...</td>\n",
       "      <td>[v, celtic, regular, season, fucked, play, pla...</td>\n",
       "      <td>[v, celtic, regular, season, fuck, play, playoff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[could, actually, kill, girl, sorry]</td>\n",
       "      <td>[could, actual, kill, girl, sorri]</td>\n",
       "      <td>[could, actually, kill, girl, sorry]</td>\n",
       "      <td>[could, actual, kill, girl, sorri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[find, hard, believe, im, afraid]</td>\n",
       "      <td>[find, hard, believ, im, afraid]</td>\n",
       "      <td>[find, hard, believe, im, afraid]</td>\n",
       "      <td>[find, hard, believ, im, afraid]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [vinco, tresorpack, difficulty, object, disass...   \n",
       "1     [glad, dot, taks, tomorrow, thankful, startho]   \n",
       "2  [vs, celtics, regular, season, fucked, play, p...   \n",
       "3               [could, actually, kill, girl, sorry]   \n",
       "4                  [find, hard, believe, im, afraid]   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [vinco, tresorpack, difficulti, object, disass...   \n",
       "1         [glad, dot, tak, tomorrow, thank, startho]   \n",
       "2  [vs, celtic, regular, season, fuck, play, play...   \n",
       "3                 [could, actual, kill, girl, sorri]   \n",
       "4                   [find, hard, believ, im, afraid]   \n",
       "\n",
       "                                              lemmed  \\\n",
       "0  [vinco, tresorpack, difficulty, object, disass...   \n",
       "1     [glad, dot, taks, tomorrow, thankful, startho]   \n",
       "2  [v, celtic, regular, season, fucked, play, pla...   \n",
       "3               [could, actually, kill, girl, sorry]   \n",
       "4                  [find, hard, believe, im, afraid]   \n",
       "\n",
       "                                                both  \n",
       "0  [vinco, tresorpack, difficulti, object, disass...  \n",
       "1         [glad, dot, tak, tomorrow, thank, startho]  \n",
       "2  [v, celtic, regular, season, fuck, play, playoff]  \n",
       "3                 [could, actual, kill, girl, sorri]  \n",
       "4                   [find, hard, believ, im, afraid]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorizer. We go with the idea that we do not want the words that appear in less than 5 tweets and in more than 80% of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vectoriser\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create a corpus. Our train set would both positive and negative, and our test set is, obviously, the unlabeled part.\n",
    "\n",
    "To do this, we will append both negative and positive DF, then create a matrix of labels for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the list of words into a usable format\n",
    "neg_DF = neg_DF[\"lemmed\"]\n",
    "pos_DF = pos_DF[\"lemmed\"]\n",
    "test_DF = test_DF[\"lemmed\"]\n",
    "neg_DF = pd.DataFrame(neg_DF)\n",
    "pos_DF = pd.DataFrame(pos_DF)\n",
    "test_DF = pd.DataFrame(test_DF)\n",
    "neg_DF[\"lemmed\"] = neg_DF.lemmed.apply(' '.join)\n",
    "pos_DF[\"lemmed\"] = pos_DF.lemmed.apply(' '.join)\n",
    "test_DF[\"lemmed\"] = test_DF.lemmed.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we thus know that all the first ones are labeled as -1 and all the others as 1\n",
    "all_labeled_DF = pd.concat([neg_DF, pos_DF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create the labels\n",
    "negs = len(neg_DF.index)\n",
    "poss = len(pos_DF.index)\n",
    "labels = np.zeros(negs+poss)\n",
    "labels[0:negs]=-1\n",
    "labels[negs:negs+poss]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_df corresponds to < documents than min_df",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-27a9176d9ea7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_corpus_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_labeled_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_corpus_tf_idf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \"\"\"\n\u001b[0;32m-> 1352\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/david/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmax_doc_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin_doc_count\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 raise ValueError(\n\u001b[0;32m--> 856\u001b[0;31m                     \"max_df corresponds to < documents than min_df\")\n\u001b[0m\u001b[1;32m    857\u001b[0m             X, self.stop_words_ = self._limit_features(X, vocabulary,\n\u001b[1;32m    858\u001b[0m                                                        \u001b[0mmax_doc_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: max_df corresponds to < documents than min_df"
     ]
    }
   ],
   "source": [
    "train_corpus_tf_idf = vectorizer.fit_transform(all_labeled_DF) \n",
    "test_corpus_tf_idf = vectorizer.transform(test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create both models\n",
    "model1 = LinearSVC() # SVM\n",
    "model2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on the given models\n",
    "model1.fit(train_corpus_tf_idf,labels)\n",
    "model2.fit(train_corpus_tf_idf,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "result1 = model1.predict(test_corpus_tf_idf)\n",
    "result2 = model2.predict(test_corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result1 and result2 are the labels predicted for the tweets we got in the test corpus. This means we probably jsute have to transforme this into a csv as it is shown in the sample submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting it to integer for prediction csv\n",
    "result1 = [int(x) for x in result1]\n",
    "result2 = [int(x) for x in result2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_df = pd.DataFrame(result1)\n",
    "svm_df['Id'] = svm_df.index + 1\n",
    "svm_df['Prediction'] = svm_df[0]\n",
    "svm_df = svm_df[['Id', 'Prediction']]\n",
    "svm_df.to_csv('svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_df = pd.DataFrame(result2)\n",
    "bayes_df['Id'] = bayes_df.index + 1\n",
    "bayes_df['Prediction'] = bayes_df[0]\n",
    "bayes_df = bayes_df[['Id', 'Prediction']]\n",
    "bayes_df.to_csv('bayes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
