{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, nltk, pickle, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu qu'on a une librarie qui nous permet de faire pas mal de choses, on va:\n",
    "- mettre tout en minuscule\n",
    "- retirer la ponctuation\n",
    "- retirer tous les nombres et caractères non alphanumériques\n",
    "- les tokeniser: donc séparer les mots\n",
    "- retirer les stop words (and, the, etc)\n",
    "- stemmer -> avoir que la racine de chaque mots\n",
    "\n",
    "à partir de là on aura déjà un dataset plus ou moins propre =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function does the job of extracting tweet data and formatting it to suit our needs\n",
    "def format_tweets(filename):\n",
    "    # Read the data file\n",
    "    with open(\"twitter-datasets/\" + filename, \"r\", encoding=\"utf8\") as myfile:\n",
    "        data = myfile.readlines()\n",
    "        \n",
    "    # Make a dataframe out of the data\n",
    "    tweets = pd.DataFrame(data)\n",
    "    \n",
    "    # These are stop words that we want to take out from the tweets\n",
    "    lang_set = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    # The replacement instructions for below\n",
    "    replacements = [\n",
    "        (\"<user>\", ''),\n",
    "        (\"<url>\", ''),\n",
    "        (\"'\", ' '),\n",
    "        (r'[^\\w\\s]', ''),\n",
    "        (r'[\\d]', ''),\n",
    "        (r'(?:^| )\\w(?:$| )', ' ')\n",
    "    ]\n",
    "        \n",
    "    # Put everything in lowercase\n",
    "    tweets[0] = tweets[0].astype(str).str.lower()\n",
    "    \n",
    "    # Take out :\n",
    "    # - usertags\n",
    "    # - urls\n",
    "    # - apostrophes\n",
    "    # - punctuation and non-alphanumerical characters\n",
    "    # - numbers\n",
    "    # - single letter words\n",
    "    for key, value in replacements:\n",
    "        tweets[0] = tweets[0].str.replace(key, value)\n",
    "        \n",
    "    # Replace blocks of 3 or more times the same letter by a single occurence of the letter\n",
    "    tweets[0] = tweets[0].apply(lambda x: re.sub(r'([a-zA-Z])\\1{2,}', r'\\1', x))\n",
    "        \n",
    "    # Tokenize each tweet\n",
    "    tweets[0] = tweets[0].str.split()\n",
    "    \n",
    "    # Remove the stop words and the \"rt\"s (retweets)\n",
    "    for item in [lang_set, 'rt']:\n",
    "        tweets[0] = tweets[0].apply(lambda tweet: [word for word in tweet if word not in item])\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = format_tweets(\"train_neg.txt\")\n",
    "positive_DF = format_tweets(\"train_pos.txt\")\n",
    "test_DF = format_tweets(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai remarké que stemmetize n'est pas toujours la meilleure chose à faire et tente du coup aussi de lemmetize. Ca nous rajoute de ce faite deux colonnes, pour qu'on puisse ensuite sélectionner la meilleure à utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function applies stemming and lemmatizing to the given tweet set\n",
    "def stem_and_lem(tweets, stemmer, lemmer):\n",
    "    tweets['stemmed'] = tweets[0].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    tweets['lemmed'] = tweets[0].apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])\n",
    "    tweets['both'] = tweets['lemmed'].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating the stemmer and lemmatizer\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english') \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les lemmetizer ne semble pas amrcher à 100% donc essayons de faire les deux (donc utiliser le stem sur le lemmetizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = stem_and_lem(negative_DF, stemmer, lemmatizer)\n",
    "positive_DF = stem_and_lem(positive_DF, stemmer, lemmatizer)\n",
    "test_DF = stem_and_lem(test_DF, stemmer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function saves the processed tweets to a txt file\n",
    "def save_tweets(tweets, filename):\n",
    "    # Put the stemmed and lemmetized tweets back to string form\n",
    "    data = tweets['both'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"twitter-datasets/\" + filename, \"w\", encoding=\"utf8\") as myfile:\n",
    "        data.to_csv(myfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tweets(negative_DF, \"train_neg_proc.txt\")\n",
    "save_tweets(positive_DF, \"train_pos_proc.txt\")\n",
    "save_tweets(test_DF, \"test_data_proc.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il va falloir faire ce qui est proposé dans le pdf, donc par exemple, compter les mots qui apparaissent le plus dans negatif et positif, et pourquoi pas utiliser ça ensuite dans notre algorithme pour décider si c'est positif ou négatif dans le test. =)\n",
    "\n",
    "Quelques petites notes: \n",
    "- chaque tweet est un liste de mots\n",
    "- on a quatre colonnes: les mots de base du tweet, les mots mais stemmés (racine du mot), les mots mais lemmés (idem mais se veut plus précis) et un qui fait les deux (lem puis stem)\n",
    "- je pars du principe que tout est en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get embeddings and index values\n",
    "emb = np.load(\"outputs/embeddings.npy\")\n",
    "\n",
    "vocab_cut = pd.read_csv(\"outputs/vocab_cut.txt\", sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "index = pd.Series(vocab_cut[vocab_cut.columns[0]].values)\n",
    "\n",
    "# Create word definition matrix\n",
    "word_weights = pd.DataFrame(data=emb, index=index)\n",
    "\n",
    "word_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function removes words that are not in the vocabulary from the tweets\n",
    "def clean_tweets(tweets, vocab):\n",
    "    clean = tweets.copy().apply(lambda tweet: [word for word in tweet if word in vocab.values])\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract just the column we need\n",
    "neg_DF = negative_DF[\"both\"]\n",
    "pos_DF = positive_DF[\"both\"]\n",
    "test_DF = test_DF[\"both\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unused</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want a sparse matrix that tells us for each tweet (rows) how many times a given\n",
    "# word (columns) appears (\"bags of words\" representation)\n",
    "\n",
    "# MIGHT NEED TO BE REDEFINED WITH neg_words AND pos_words INSTEAD !!!!!!\n",
    "def bags_of_words(tweets=None):\n",
    "    \n",
    "    # We get the vocabulary\n",
    "    with open('outputs/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    col = 0\n",
    "    data, rows, cols = [], [], []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # We count word occurences in a tweet\n",
    "        word_count = Counter(tweet).items()\n",
    "        \n",
    "        # If the word is in the vocabulary we add it in the matrix\n",
    "        for word, count in word_count:\n",
    "            row = vocab.get(word)\n",
    "            \n",
    "            if row:\n",
    "                data.append(count)\n",
    "                cols.append(col)\n",
    "                rows.append(row)\n",
    "        \n",
    "        col += 1\n",
    "        \n",
    "    # We convert the scipy.sparse matrix to pandas.SparseSeries for ease of use\n",
    "    return pd.SparseSeries.from_coo(sparse.coo_matrix((data, (rows, cols))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bags = bags_of_words(pos_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sums the embeddings of each word in the given tweet\n",
    "\n",
    "# THIS METHOD IS NOT OPTIMAL AND WOULD BENEFIT BEING MADE FASTER UNLESS WE DO NOT USE IT IN THE END\n",
    "def query_weights(tweet):\n",
    "    w = pd.DataFrame(columns=range(20))\n",
    "    \n",
    "    for word in tweet:\n",
    "        try:\n",
    "            w = w.append(word_weights.loc[word, :])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return w.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tweet embeddings\n",
    "neg_dims = neg_DF.copy().apply(query_weights)\n",
    "pos_dims = pos_DF.copy().apply(query_weights)\n",
    "test_dims = test_DF.copy().apply(query_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the embeddings in pkl files\n",
    "with open('outputs/neg_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_dims, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('outputs/pos_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_dims, f, pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "with open('outputs/test_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dims, f, pickle.HIGHEST_PROTOCOL)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the embeddings from pkl files\n",
    "with open('outputs/neg_dims.pkl', 'rb') as f:\n",
    "    neg_dims = pickle.load(f)\n",
    "    \n",
    "with open('outputs/pos_dims.pkl', 'rb') as f:\n",
    "    pos_dims = pickle.load(f)\n",
    "    \n",
    "with open('outputs/test_dims.pkl', 'rb') as f:\n",
    "    test_dims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the matrices for SVM fitting, we just put the positive and negative embeddings together and\n",
    "# create the appropriate y matrix with 1's and -1's\n",
    "X = pos_dims.append(neg_dims)\n",
    "ones = np.ones((pos_dims.shape[0], 1))\n",
    "y = np.append(ones, -1 * ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applies the Random Forest Classifier technique to the data\n",
    "start = time.time()\n",
    "clf = RandomForestClassifier(min_samples_leaf=20)\n",
    "clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Random Forest\", end - start, clf.score(X, y))\n",
    "pred = pd.DataFrame(clf.predict(test_dims))\n",
    "pred.columns = [\"Prediction\"]\n",
    "pred.insert(0, \"Id\", pred.index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We save the submission\n",
    "pred.to_csv(\"outputs/submission.csv\", index=False, float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply cross validation to the data\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vectoriser\n",
    "neg_vectorizer = TfidfVectorizer()\n",
    "pos_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put the list of words into a usable format\n",
    "neg_DF = pd.DataFrame(neg_DF)\n",
    "pos_DF = pd.DataFrame(pos_DF)\n",
    "neg_DF[\"both\"] = neg_DF.both.apply(' '.join)\n",
    "pos_DF[\"both\"] = pos_DF.both.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train each vectorizer on the respective corpus\n",
    "x_neg = neg_vectorizer.fit_transform(neg_DF[\"both\"])\n",
    "x_pos = pos_vectorizer.fit_transform(pos_DF[\"both\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "\n",
    "lda_neg = LatentDirichletAllocation(n_components=n_components, max_iter=5, \n",
    "                                learning_method='online', learning_offset=50., random_state=0)\n",
    "lda_pos = LatentDirichletAllocation(n_components=n_components, max_iter=5, \n",
    "                                learning_method='online', learning_offset=50., random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_neg.fit(x_neg)\n",
    "lda_pos.fit(x_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function given by tfidf tutorial on sklearn\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_top_words = 20\n",
    "\n",
    "tf_neg_features = neg_vectorizer.get_feature_names()\n",
    "print_top_words(lda_neg, tf_neg_features, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_pos_features = pos_vectorizer.get_feature_names()\n",
    "print_top_words(lda_pos, tf_pos_features, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je suis pas sûre que ce soit hyper utile ou de ce qu'il faille faire avec le TF-IDF en faite."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
