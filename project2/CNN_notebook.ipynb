{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutional Neural Network (CNN) Implementation</h1>\n",
    "\n",
    "In this notebook we create a CNN to make predictions on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed general imports\n",
    "import os, re\n",
    "import numpy as np\n",
    "\n",
    "# Helper code\n",
    "from Keras.helpers import load_data_and_labels, clean_files, submission, embedding_matrix\n",
    "\n",
    "# Libraries from Keras to implement CNN\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Merge, Convolution1D, Dropout, AveragePooling1D\n",
    "from keras.layers.core import Flatten\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defnine the path where the glove twitter dataset is\n",
    "TWITTER_GLOVE_PATH='scripts/glove.twitter.27B.200d.txt' \n",
    "nb_word = 20000\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "     #  Delete url and user \n",
    "    string = re.sub(r'<user>', ' ', string)\n",
    "    string = re.sub(r'<url>', ' ', string)\n",
    "    #  Replaced smiley with meaning\n",
    "    string = re.sub(r'\\:\\)', ' happy ', string)\n",
    "    string = re.sub(r'\\:\\(', ' sad ', string)\n",
    "    string = re.sub(r'\\:\\/', ' sarcasm ', string)\n",
    "    string = re.sub(r'\\<\\d', ' love ', string)\n",
    "    string = re.sub(r'&', ' and ', string)\n",
    "    # Change the conjugation\n",
    "    string = re.sub(r\"what's \", \"what is \", string)\n",
    "    string = re.sub(r\" \\'s \", \" is \", string)\n",
    "    string = re.sub(r\" \\'ve \", \" have \", string)\n",
    "    string = re.sub(r\"can't \", \"cannot \", string)\n",
    "    string = re.sub(r\"n't \", \" not \", string)\n",
    "    string = re.sub(r\"i'm \", \" i am \", string)\n",
    "    string = re.sub(r\"i've \", \" i have \", string)\n",
    "    string = re.sub(r\"youre \", \" you are \", string)\n",
    "    string = re.sub(r\"it's \", \" it is \", string)\n",
    "    string = re.sub(r\"\\'re \", \" are \", string)\n",
    "    string = re.sub(r\"\\'d \", \" would \", string)\n",
    "    string = re.sub(r\"\\'ll \", \" will \", string)\n",
    "    string = re.sub(r\"don't \", \" dont \", string)\n",
    "    string = re.sub(r\"im \", \" i am \", string)\n",
    "    string = re.sub(r\"do no \", \" dont \", string)\n",
    "    string = re.sub(r\"does no \", \" dont \", string)\n",
    "    string = re.sub(r\"are no \", \" arent \", string)\n",
    "    string = re.sub(r\"is no \", \" isnt \", string)\n",
    "    string = re.sub(r\"am no \", \" arent \", string)\n",
    "    string = re.sub(r\"its no \", \" it isnt \", string)\n",
    "    string = re.sub(r\"did no \", \" didnt \", string)\n",
    "    string = re.sub(r\"i no \", \" i arent \", string)\n",
    "    string = re.sub(r\"will no \", \" wont \", string)\n",
    "    string = re.sub(r\"have no \", \" havent \", string)\n",
    "    string = re.sub(r\"don t \", \" dont \", string)\n",
    "    string = re.sub(r\"doesn t \", \" dont \", string)\n",
    "    string = re.sub(r\"aren t \", \" arent \", string)\n",
    "    string = re.sub(r\"isn t \", \" isnt \", string)\n",
    "    string = re.sub(r\"didn t \", \" didnt \", string)\n",
    "    string = re.sub(r\"won t \", \" wont \", string)\n",
    "    string = re.sub(r\"haven t \", \" havent \", string)\n",
    "    # change the ponctuation \n",
    "    string = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", string)\n",
    "    string = re.sub(r\"\\d\", \" \", string) \n",
    "    string = re.sub(r\",\", \" \", string)\n",
    "    string = re.sub(r\"\\.\", \" \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\/\", \" \", string)\n",
    "    string = re.sub(r\"\\^\", \" ^ \", string)\n",
    "    string = re.sub(r\"\\+\", \" + \", string)\n",
    "    string = re.sub(r\"\\-\", \" - \", string)\n",
    "    string = re.sub(r\"\\=\", \" = \", string)\n",
    "    string = re.sub(r\"'\", \" \", string)\n",
    "    string = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", string)\n",
    "    string = re.sub(r\":\", \" : \", string)\n",
    "    string = re.sub(r\" e g \", \" eg \", string)\n",
    "    string = re.sub(r\" b g \", \" bg \", string)\n",
    "    string = re.sub(r\" u s \", \" american \", string)\n",
    "    string = re.sub(r\"\\0s\", \"0\", string)\n",
    "    string = re.sub(r\" 9 11 \", \"911\", string)\n",
    "    string = re.sub(r\"e - mail\", \"email\", string)\n",
    "    string = re.sub(r\"j k\", \"jk\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "   \n",
    "    # return all in lowercase \n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the cleaned version of the tweet files\n",
    "def clean_files():\n",
    "    positive_examples = list(open('twitter-datasets/train_pos_full.txt', \"r\", encoding=\"utf-8\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open('twitter-datasets/train_neg_full.txt', \"r\", encoding=\"utf-8\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    test_examples = list(open('twitter-datasets/test_data.txt', \"r\", encoding=\"utf-8\").readlines())\n",
    "    test_examples = [s.strip() for s in test_examples]\n",
    "    \n",
    "    # Split by words\n",
    "    positive_string = [clean_str(sent) for sent in positive_examples]\n",
    "    negative_string = [clean_str(sent) for sent in negative_examples]\n",
    "    test_string = [clean_str(sent) for sent in test_examples]\n",
    "\n",
    "    # save the cleaned files for future use\n",
    "    with open('processed/train_pos_CNN_full.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for sent in positive_string:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open('processed/train_neg_CNN_full.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for sent in negative_string:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open('processed/test_CNN.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for sent in test_string:\n",
    "            f.write(sent + '\\n')\n",
    "            \n",
    "# Clean the file if it does not exist\n",
    "if not os.path.exists('processed/train_pos_CNN_full.txt') \\\n",
    "    or not os.path.exists('../processed/train_neg_CNN_full.txt'):\n",
    "        print('Cleaned CNN files do not exist')\n",
    "        clean_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels for the training sets and split sentences for the testing set\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\", encoding=\"utf-8\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\", encoding=\"utf-8\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    test = list(open(test_data_file, \"r\", encoding=\"utf-8\").readlines())\n",
    "    test = [s.strip() for s in test]\n",
    "    \n",
    "    # Split by words\n",
    "    train = positive_examples + negative_examples\n",
    "    \n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    labels = np.array(labels)\n",
    "    return [train, labels, test]\n",
    "\n",
    "# Load data from processed files\n",
    "train,labels,test=load_data_and_labels('processed/train_pos_CNN_full.txt',\n",
    "                                       'processed/train_neg_CNN_full.txt',\n",
    "                                       'processed/test_CNN.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data\n",
    "In this part, we tokenize the data, which means we replace words by numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize the text samples into a 2D integer tensor with Tokenizer\n",
    "tokenizer = Tokenizer(num_words=nb_word)\n",
    "tokenizer.fit_on_texts(train)\n",
    "sequences_train = tokenizer.texts_to_sequences(train)\n",
    "sequences_test = tokenizer.texts_to_sequences(test)\n",
    "# take only the index of words\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix(path_glove_twitter,word_index,nb_words,embedding_dim):\n",
    "    # create index mapping words in the embeddings  to their embedding vector\n",
    "    embeddings_index = {}\n",
    "    f = open(path_glove_twitter, \"r\", encoding=\"utf-8\") \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        # for each word we find the corresponding word vector\n",
    "        embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "    f.close()\n",
    "\n",
    "    # Create the embedding matrix corresponding to our dataset\n",
    "    embedding_matrix = np.zeros((nb_words + 1,embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > nb_words: \n",
    "            continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in the embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "#create the embedding matrix that will be the weight of our embedding layer\n",
    "print('create embedding_matrix')\n",
    "embedding_matrix_200 = embedding_matrix(TWITTER_GLOVE_PATH, word_index, nb_word,embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use pad_sequences to put every tweets at the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#put at the same length every sentence (the length is the max length of all tweets)\n",
    "sequence_lenght = max(len(x) for x in train)\n",
    "xtrain = pad_sequences(sequences_train, maxlen=sequence_lenght)\n",
    "xtest = pad_sequences(sequences_test, maxlen=sequence_lenght)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us randomize and split the data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split data into training and testing\n",
    "num_row = len(labels)\n",
    "indices = np.random.permutation(num_row)\n",
    "train = xtrain[indices]\n",
    "ytrain=labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_tweets_train=200000\n",
    "# add a part of the tweet to have external accuracy\n",
    "nb_tweets_test=10000\n",
    "validation_split = int(0.20*nb_tweets_train)\n",
    "\n",
    "x_train=xtrain[: nb_tweets_train]\n",
    "y_train=ytrain[: nb_tweets_train]\n",
    "x_validation=X_train[nb_tweets_train+1 : nb_tweets_train+validation_split]\n",
    "y_validation=Y_train[nb_tweets_train+1 : nb_tweets_train+validation_split]\n",
    "x_test=X_train[nb_tweets_train+validation_split+1 :nb_tweets_train+validation_split+nb_tweets_test]\n",
    "y_test=Y_train[nb_tweets_train+validation_split+1 :nb_tweets_train+validation_split+nb_tweets_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the CNN\n",
    "In this part, we create the actual CNN as discussed in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters = [2,3,4]\n",
    "num_filters = 120\n",
    "drop = 0.6\n",
    "nb_epoch = 3\n",
    "batch_size = 60\n",
    "\n",
    "convolutions = []\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=nb_words + 1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix_200],\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=False)(inputs)\n",
    "embedding2 = Embedding(input_dim=nb_words + 1,\n",
    "                            output_dim=embedding_dim,\n",
    "                            weights=[embedding_matrix_200],\n",
    "                            input_length=sequence_length,\n",
    "                            trainable=True)(inputs)\n",
    "for nb_filter in filters:\n",
    "    conv = Convolution1D(num_filters, nb_filter, activation='relu')(embedding)\n",
    "    maxpooling = AveragePooling1D(3)(conv)\n",
    "    flatten=Flatten()(maxpooling)\n",
    "    convolutions.append(flatten)\n",
    "for nb_filter in filters:\n",
    "    conv = Convolution1D(num_filters, nb_filter, activation='relu')(embedding2)\n",
    "    maxpooling = AveragePooling1D(3)(conv)\n",
    "    flatten=Flatten()(maxpooling)\n",
    "    convolutions.append(flatten)\n",
    "\n",
    "merged_tensor = Merge(mode='concat', concat_axis=1)(convolutions)\n",
    "dense0=Dense(120,init='uniform', activation='relu')(merged_tensor)\n",
    "dropout0 = Dropout(drop)(dense0)\n",
    "dense1=Dense(60,init='uniform', activation='relu')(dropout0)\n",
    "dropout1 = Dropout(drop)(dense1)\n",
    "out = Dense(output_dim=2, init='uniform',activation='softmax')(dropout1)\n",
    "\n",
    "model = Model(input=inputs, output=out)\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0005)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                                          optimizer=Adam,\n",
    "                                          metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=nb_epoch ,\n",
    "          batch_size=batch_size,\n",
    "          validation_data=(x_validation, y_validation))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
