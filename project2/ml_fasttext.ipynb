{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>FastText Implementation</h1>\n",
    "\n",
    "In this notebook we apply FastText to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed general imports\n",
    "import csv, time, os\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for FastText\n",
    "import fasttext\n",
    "from scipy.sparse import *\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading Data</h3>\n",
    "\n",
    "This function loads data from the processed tweet files, splits the data into words and generates labels. Returns split sentences and labels for the training sets and split sentences for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, test_data_file):\n",
    "    \n",
    "    # Load data from files\n",
    "    with open(\"twitter-datasets/\" + positive_data_file, \"r\", encoding=\"utf8\") as myfile:\n",
    "        pos_data = [s.strip() for s in list(myfile.readlines())]\n",
    "        \n",
    "    with open(\"twitter-datasets/\" + negative_data_file, \"r\", encoding=\"utf8\") as myfile:\n",
    "        neg_data = [s.strip() for s in list(myfile.readlines())]\n",
    "        \n",
    "    with open(\"twitter-datasets/\" + test_data_file, \"r\", encoding=\"utf8\") as myfile:\n",
    "        test_data = [s.strip() for s in list(myfile.readlines())]\n",
    "    \n",
    "    # Concatenate data\n",
    "    train_data = pos_data + neg_data\n",
    "    \n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in pos_data]\n",
    "    negative_labels = [-1 for _ in neg_data]\n",
    "    \n",
    "    labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    \n",
    "    return train_data, labels, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification</h3>\n",
    "\n",
    "Here we run our classification. We first load the datasets, then save the training set with the labels appended. We feed the resulting file to FastText and get a classifier, which we use to predict labels for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start computing time \n",
    "start = time.time()\n",
    "\n",
    "# Load data from processed files\n",
    "train, labels, test = load_data_and_labels('train_pos_proc.txt', 'train_neg_proc.txt', 'test_data.txt')\n",
    "\n",
    "# Create the correct label in front of every tweets as : '__label__<X>'\n",
    "with open('../outputs/fastText_labels.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for sent, label in zip(train ,labels):\n",
    "        f.write('__label__' + str(label) + ' ' + sent + '\\n')\n",
    "\n",
    "# define the parameters for the fastText classifier\n",
    "window, epochs = 10, 10\n",
    "\n",
    "# Build the fastText classifier \n",
    "classifier = fasttext.supervised('outputs/fastText_labels.txt', 'model', label_prefix='__label__', ws=window, epoch=epochs)\n",
    "\n",
    "# Create the prediction\n",
    "prediction = classifier.predict_proba(test)\n",
    "\n",
    "# Compute the computing time\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Validation</h3>\n",
    "\n",
    "We can now test the accuracy of our classifier by running a 10-fold cross validation on it. With the same parameters as before, we split out data in 10 scrambled subsets, with one of them acting as a testing set in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from the processed files\n",
    "train, labels, test = load_data_and_labels('train_pos_proc.txt', 'train_neg_proc.txt', 'test_data.txt')\n",
    "\n",
    "# define the parameters for the fastText classifier\n",
    "window, epochs = 10, 10\n",
    "\n",
    "# create random indices of the rows size\n",
    "num_row = len(labels)\n",
    "indices = np.random.permutation(num_row)\n",
    "\n",
    "# Define the number of folds for the cross-validation\n",
    "fold = 10;\n",
    "k_fold = KFold(n_splits=fold)\n",
    "accuracy = np.zeros((fold))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(labels):\n",
    "    \n",
    "    # Randomize the cross-val indices with the indices created above\n",
    "    train_indices = indices[train_indices]\n",
    "    test_indices = indices[test_indices]\n",
    "    \n",
    "    # Create the correct label in front of every tweets as : '__label__<X>'\n",
    "    # For the training set\n",
    "    with open('outputs/fastText_train_labels.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for indice in train_indices:\n",
    "             f.write('__label__' + str(labels[indice]) + ' ' + train[indice] + '\\n')\n",
    "                \n",
    "    # For the testing set \n",
    "    with open('outputs/fastText_test_labels.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for indice in test_indices:\n",
    "             f.write('__label__'  + str(labels[indice]) + ' ' + train[indice] + '\\n')\n",
    "    \n",
    "    # Build the fastText classifier \n",
    "    classifier = fasttext.supervised('outputs/fastText_train_labels.txt', 'model_cros_val', label_prefix='__label__', ws=window, epoch=epochs)\n",
    "    \n",
    "    # Evaluate how the classifier performs on the testing set\n",
    "    result = classifier.test('outputs/fastText_test_labels.txt')\n",
    "    \n",
    "    # Saving the accuracy for every iteration\n",
    "    accuracy[i] = result.precision\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the accuracy of each of the folds of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission for FastText</h3>\n",
    "\n",
    "This code generates a submission file for the FastText implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('outputs/sub_fasttext.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Id', 'Prediction']\n",
    "    sub_writer = csv.DictWriter(csvfile, fieldnames)\n",
    "    index = 0\n",
    "    sub_writer.writeheader()\n",
    "    \n",
    "    for pred in prediction:\n",
    "        index += 1\n",
    "        sub_writer.writerow({'Id': str(index), 'Prediction': str(pred[0][0])})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
