{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>FastText Implementation</h1>\n",
    "\n",
    "In this notebook we apply FastText to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needed general imports\n",
    "import csv, fasttext, time, os\n",
    "import numpy as np\n",
    "\n",
    "# Helper code\n",
    "from fastText.helpers import clean_str\n",
    "\n",
    "# Libraries for FastText\n",
    "import fasttext\n",
    "from scipy.sparse import *\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Loading Data</h3>\n",
    "\n",
    "This function loads data from the processed tweet files, splits the data into words and generates labels. Returns split sentences and labels for the training sets and split sentences for the testing set. The function after that cleans the files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_and_labels(positive_data_file, negative_data_file, test_data_file):\n",
    "    \"\"\"\n",
    "    Loads data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels for the training sets and split sentences for the testing set\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\", encoding=\"utf-8\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    \n",
    "    negative_examples = list(open(negative_data_file, \"r\", encoding=\"utf-8\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    \n",
    "    test = list(open(test_data_file, \"r\", encoding=\"utf-8\").readlines())\n",
    "    test = [s.strip() for s in test]\n",
    "    \n",
    "    # Split by words\n",
    "    train = positive_examples + negative_examples\n",
    "    \n",
    "    # Generate labels\n",
    "    positive_labels = [1 for _ in positive_examples]\n",
    "    negative_labels = [-1 for _ in negative_examples]\n",
    "    labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [train, labels, test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_files():\n",
    "    positive_examples = list(open('twitter-datasets/train_pos_full.txt', \"r\", encoding=\"utf-8\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    \n",
    "    negative_examples = list(open('twitter-datasets/train_neg_full.txt', \"r\", encoding=\"utf-8\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    \n",
    "    test_examples = list(open('twitter-datasets/test_data.txt', \"r\", encoding=\"utf-8\").readlines())\n",
    "    test_examples = [s.strip() for s in test_examples]\n",
    "    \n",
    "    # process every word\n",
    "    positive_string = [clean_str(sent) for sent in positive_examples]\n",
    "    negative_string = [clean_str(sent) for sent in negative_examples]\n",
    "    test_string = [clean_str(sent) for sent in test_examples]\n",
    "\n",
    "    with open('processed/train_pos_fastText_full.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for sent in positive_string:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open('processed/train_neg_fastText_full.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for sent in negative_string:\n",
    "            f.write(sent + '\\n')\n",
    "\n",
    "    with open('processed/test_data_fastText.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for sent in test_string:\n",
    "            f.write(sent + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Classification</h3>\n",
    "\n",
    "Here we run our classification. We first load the datasets, then save the training set with the labels appended. We feed the resulting file to FastText and get a classifier, which we use to predict labels for the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start computing time \n",
    "start = time.time()\n",
    "\n",
    "# Clean file if it does not exist\n",
    "if not os.path.exists('processed/train_pos_fastText_full.txt') \\\n",
    "    or not os.path.exists('processed/train_neg_fastText_full.txt'):\n",
    "        print('Clean fastText files did not exist')\n",
    "        clean_files()\n",
    "\n",
    "# Load data from processed files\n",
    "train, labels, test = load_data_and_labels('processed/train_pos_fastText_full.txt', 'processed/train_neg_fastText_full.txt', \n",
    "                                           'processed/test_data_fastText.txt')\n",
    "\n",
    "# Create the correct label in front of every tweets as : '__label__<X>'\n",
    "with open('outputs/fastText_labels.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for sent, label in zip(train ,labels):\n",
    "        f.write('__label__' + str(label) + ' ' + sent + '\\n')\n",
    "\n",
    "# define the parameters for the fastText classifier\n",
    "window, epochs = 10, 10\n",
    "\n",
    "# Build the fastText classifier \n",
    "classifier = fasttext.supervised('outputs/fastText_labels.txt', 'model', label_prefix='__label__', ws=window, epoch=epochs)\n",
    "\n",
    "# Create the prediction\n",
    "prediction = classifier.predict_proba(test)\n",
    "\n",
    "# Compute the computing time\n",
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Cross-Validation</h3>\n",
    "\n",
    "We can now test the accuracy of our classifier by running a 10-fold cross validation on it. With the same parameters as before, we split out data in 10 scrambled subsets, with one of them acting as a testing set in each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load data from processed files\n",
    "train, labels, test = load_data_and_labels('train_pos_proc.txt', 'train_neg_proc.txt', 'test_data.txt')\n",
    "\n",
    "# define the parameters for the fastText classifier\n",
    "window, epochs = 10, 10\n",
    "\n",
    "# create random indices of the rows size\n",
    "num_row = len(labels)\n",
    "indices = np.random.permutation(num_row)\n",
    "\n",
    "# Define the number of folds for the cross-validation\n",
    "fold = 10;\n",
    "k_fold = KFold(n_splits=fold)\n",
    "accuracy = np.zeros((fold))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(labels):\n",
    "    # Randomize the cross-val indices with the indices created above\n",
    "    train_indices = indices[train_indices]\n",
    "    test_indices = indices[test_indices]\n",
    "    \n",
    "    # Create the correct label in front of every tweets as : '__label__<X>'\n",
    "    # For the training set\n",
    "    with open('outputs/fastText_train_labels.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for indice in train_indices:\n",
    "             f.write('__label__' + str(labels[indice]) + ' ' + train[indice] + '\\n')\n",
    "                \n",
    "    # For the testing set \n",
    "    with open('outputs/fastText_test_labels.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        for indice in test_indices:\n",
    "             f.write('__label__'  + str(labels[indice]) + ' ' + train[indice] + '\\n')\n",
    "    \n",
    "    # Build the fastText classifier \n",
    "    classifier = fasttext.supervised('outputs/fastText_train_labels.txt', 'model_cros_val', label_prefix='__label__', ws=window, epoch=epochs)\n",
    "    \n",
    "    # Evaluate how the classifier performs on the testing set\n",
    "    result = classifier.test('outputs/fastText_test_labels.txt')\n",
    "    \n",
    "    # Saving the accuracy for every iteration\n",
    "    accuracy[i] = result.precision\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now view the accuracy of each of the folds of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Submission for FastText</h3>\n",
    "\n",
    "This code generates a submission file for the FastText implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('outputs/sub_fasttext.csv', 'w') as csvfile:\n",
    "    fieldnames = ['Id', 'Prediction']\n",
    "    sub_writer = csv.DictWriter(csvfile, fieldnames)\n",
    "    index = 0\n",
    "    sub_writer.writeheader()\n",
    "    \n",
    "    for pred in prediction:\n",
    "        index += 1\n",
    "        sub_writer.writerow({'Id': str(index), 'Prediction': str(pred[0][0])})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
