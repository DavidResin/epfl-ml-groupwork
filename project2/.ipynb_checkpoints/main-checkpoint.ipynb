{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Natural language processing library.\n",
    "import csv, nltk, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "\n",
    "# need to use once to download nltk on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu qu'on a une librarie qui nous permet de faire pas mal de choses, on va:\n",
    "- mettre tout en minuscule\n",
    "- retirer la ponctuation\n",
    "- retirer tous les nombres et caractères non alphanumériques\n",
    "- les tokeniser: donc séparer les mots\n",
    "- retirer les stop words (and, the, etc)\n",
    "- stemmer -> avoir que la racine de chaque mots\n",
    "\n",
    "à partir de là on aura déjà un dataset plus ou moins propre =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each new line will be a new element of array\n",
    "with open(\"twitter-datasets/train_neg_full.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    data_neg = myfile.readlines()\n",
    "with open(\"twitter-datasets/train_pos_full.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    data_pos = myfile.readlines()\n",
    "with open(\"twitter-datasets/test_data.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    data_test = myfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform array into Dataframe to use it easily\n",
    "negative_DF = pd.DataFrame(data_neg)\n",
    "positive_DF = pd.DataFrame(data_pos)\n",
    "test_DF = pd.DataFrame(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put everything to lowercase.\n",
    "negative_DF[0] = negative_DF[0].astype(str).str.lower()\n",
    "positive_DF[0] = positive_DF[0].astype(str).str.lower()\n",
    "test_DF[0] = test_DF[0].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take out user and url\n",
    "negative_DF[0] = negative_DF[0].str.replace(\"<user>\", '')\n",
    "negative_DF[0] = negative_DF[0].str.replace(\"<url>\", '')\n",
    "\n",
    "positive_DF[0] = positive_DF[0].str.replace(\"<user>\", '')\n",
    "positive_DF[0] = positive_DF[0].str.replace(\"<url>\", '')\n",
    "\n",
    "test_DF[0] = test_DF[0].str.replace(\"<user>\", '')\n",
    "test_DF[0] = test_DF[0].str.replace(\"<url>\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take out punctuation and non alphanumerical characters\n",
    "negative_DF[0] = negative_DF[0].str.replace(r'[^\\w\\s]', '')\n",
    "positive_DF[0] = positive_DF[0].str.replace(r'[^\\w\\s]', '')\n",
    "test_DF[0] = test_DF[0].str.replace(r'[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take out numbers\n",
    "negative_DF[0] = negative_DF[0].str.replace(r'[\\d]', '')\n",
    "positive_DF[0] = positive_DF[0].str.replace(r'[\\d]', '')\n",
    "test_DF[0] = test_DF[0].str.replace(r'[\\d]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize each one.\n",
    "negative_DF[0] = negative_DF[0].str.split()\n",
    "positive_DF[0] = positive_DF[0].str.split()\n",
    "test_DF[0] = test_DF[0].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we remove the stop words\n",
    "lang_set = nltk.corpus.stopwords.words('english')\n",
    "negative_DF[0] = negative_DF[0].apply(lambda x: [word for word in x if word not in lang_set])\n",
    "positive_DF[0] = positive_DF[0].apply(lambda x: [word for word in x if word not in lang_set])\n",
    "test_DF[0] = test_DF[0].apply(lambda x: [word for word in x if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we remove the rt (retweets)\n",
    "negative_DF[0] = negative_DF[0].apply(lambda x: [word for word in x if word not in ['rt']])\n",
    "positive_DF[0] = positive_DF[0].apply(lambda x: [word for word in x if word not in ['rt']])\n",
    "test_DF[0] = test_DF[0].apply(lambda x: [word for word in x if word not in ['rt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai remarké que stemmetize n'est pas toujours la meilleure chose à faire et tente du coup aussi de lemmetize. Ca nous rajoute de ce faite deux colonnes, pour qu'on puisse ensuite sélectionner la meilleure à utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we stemmatize (get the root only) for each word\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')    \n",
    "negative_DF['stemmed'] = negative_DF[0].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "positive_DF['stemmed'] = positive_DF[0].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_DF['stemmed'] = test_DF[0].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "negative_DF['lemmed'] = negative_DF[0].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "positive_DF['lemmed'] = positive_DF[0].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "test_DF['lemmed'] = test_DF[0].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les lemmetizer ne semble pas amrcher à 100% donc essayons de faire les deux (donc utiliser le stem sur le lemmetizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF['both'] = negative_DF['lemmed'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "positive_DF['both'] = positive_DF['lemmed'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_DF['both'] = test_DF['lemmed'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the stemmed and lemmetized column and formatting to write it to a file\n",
    "neg_final = negative_DF['both'].apply(lambda x: ' '.join(x))\n",
    "pos_final = positive_DF['both'].apply(lambda x: ' '.join(x))\n",
    "test_final = test_DF['both'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write tweets to files\n",
    "with open(\"twitter-datasets/train_neg_proc.txt\", \"w\", encoding=\"utf8\") as myfile:\n",
    "    neg_final.to_csv(myfile, index=False)\n",
    "with open(\"twitter-datasets/train_pos_proc.txt\", \"w\", encoding=\"utf8\") as myfile:\n",
    "    pos_final.to_csv(myfile, index=False)\n",
    "with open(\"twitter-datasets/test_data_proc.txt\", \"w\", encoding=\"utf8\") as myfile:\n",
    "    test_final.to_csv(myfile, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il va falloir faire ce qui est proposé dans le pdf, donc par exemple, compter les mots qui apparaissent le plus dans negatif et positif, et pourquoi pas utiliser ça ensuite dans notre algorithme pour décider si c'est positif ou négatif dans le test. =)\n",
    "\n",
    "Quelques petites notes: \n",
    "- chaque tweet est un liste de mots\n",
    "- on a quatre colonnes: les mots de base du tweet, les mots mais stemmés (racine du mot), les mots mais lemmés (idem mais se veut plus précis) et un qui fait les deux (lem puis stem)\n",
    "- je pars du principe que tout est en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and index values\n",
    "emb = np.load(\"outputs/embeddings.npy\")\n",
    "index = pd.read_csv(\"outputs/vocab_cut.txt\", sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "index = index[index.columns[0]].values\n",
    "\n",
    "# Create word definition matrix\n",
    "word_weights = pd.DataFrame(data=emb, index=index)\n",
    "\n",
    "word_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function removes words that are not in the vocabulary from the tweets\n",
    "def clean_tweets(tweets, vocab):\n",
    "    tweets.apply(lambda tweet: [word for word if vocab.str.contains(word)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>FastText</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neg_DF = negative_DF[\"both\"]\n",
    "pos_DF = positive_DF[\"both\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want a sparse matrix that tells us for each tweet (rows) how many times a given\n",
    "# word (columns) appears (\"bags of words\" representation)\n",
    "def bags_of_words(tweets=None):\n",
    "    \n",
    "    # We get the vocabulary\n",
    "    with open('outputs/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    col = 0\n",
    "    data, rows, cols = [], [], []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # We count word occurences in a tweet\n",
    "        word_count = Counter(tweet).items()\n",
    "        \n",
    "        # If the word is in the vocabulary we add it in the matrix\n",
    "        for word, count in word_count:\n",
    "            row = vocab.get(word)\n",
    "            \n",
    "            if row:\n",
    "                data.append(count)\n",
    "                cols.append(col)\n",
    "                rows.append(row)\n",
    "        \n",
    "        col += 1\n",
    "        \n",
    "    # We convert the scipy.sparse matrix to pandas.SparseSeries for ease of use\n",
    "    return pd.SparseSeries.from_coo(sparse.coo_matrix((data, (rows, cols))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bags = bags_of_words(pos_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bags.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>SVM</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = pd.DataFrame([0] * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tweet embeddings\n",
    "neg_dims = pd.DataFrame(0, index=neg_DF.index, columns=word_weights.columns)\n",
    "neg_dims = neg_DF.apply(lambda tweet: word_weights.loc[tweet, :].sum(axis=0))\n",
    "neg_dims.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
