{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, nltk, pickle, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu qu'on a une librarie qui nous permet de faire pas mal de choses, on va:\n",
    "- mettre tout en minuscule\n",
    "- retirer la ponctuation\n",
    "- retirer tous les nombres et caractères non alphanumériques\n",
    "- les tokeniser: donc séparer les mots\n",
    "- retirer les stop words (and, the, etc)\n",
    "- stemmer -> avoir que la racine de chaque mots\n",
    "\n",
    "à partir de là on aura déjà un dataset plus ou moins propre =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function extracts tweet data and puts it in a dataframe\n",
    "def get_tweets(filename):\n",
    "    \n",
    "    # Read the data file\n",
    "    with open(\"twitter-datasets/\" + filename, \"r\", encoding=\"utf8\") as myfile:\n",
    "        data = myfile.readlines()\n",
    "        \n",
    "    # Make a dataframe out of the data\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = get_tweets(\"train_neg_full.txt\")\n",
    "positive_DF = get_tweets(\"train_pos_full.txt\")\n",
    "test_DF = get_tweets(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vinco tresorpack 6 ( difficulty 10 of 10 objec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>glad i dot have taks tomorrow ! ! #thankful #s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-3 vs celtics in the regular season = were fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;user&gt; i could actually kill that girl i'm so ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;user&gt; &lt;user&gt; &lt;user&gt; i find that very hard to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wish i could be out all night tonight ! &lt;user&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;user&gt; i got kicked out the wgm\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>rt &lt;user&gt; &lt;user&gt; &lt;user&gt; yes she is ! u tell it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>why is she so perfect &lt;url&gt;\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;user&gt; hi harry ! did u havea good time in aus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  vinco tresorpack 6 ( difficulty 10 of 10 objec...\n",
       "1  glad i dot have taks tomorrow ! ! #thankful #s...\n",
       "2  1-3 vs celtics in the regular season = were fu...\n",
       "3  <user> i could actually kill that girl i'm so ...\n",
       "4  <user> <user> <user> i find that very hard to ...\n",
       "5   wish i could be out all night tonight ! <user>\\n\n",
       "6                  <user> i got kicked out the wgm\\n\n",
       "7  rt <user> <user> <user> yes she is ! u tell it...\n",
       "8                      why is she so perfect <url>\\n\n",
       "9  <user> hi harry ! did u havea good time in aus..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function does the job of formatting tweet data to suit our needs\n",
    "def format_tweets(tweets):\n",
    "    \n",
    "    # These are stop words that we want to take out from the tweets\n",
    "    lang_set = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Put everything in lowercase\n",
    "    tweets[0] = tweets[0].astype(str).str.lower()\n",
    "    \n",
    "    # The replacement instructions for below, which:\n",
    "    # - remove usertags\n",
    "    # - remove urls\n",
    "    # - remove retweets (\"rt\")\n",
    "    # - replace anything that is not letters by a space\n",
    "    # - remove lone characters\n",
    "    # - replace sequences of 3 times the same letter or more by a single occurence of the character\n",
    "    replacements = [\n",
    "        (\"<user>\", ''),\n",
    "        (\"<url>\", ''),\n",
    "        (r'\\brt\\b', ''),\n",
    "        (r'[^a-z]+', ' '),\n",
    "        (r'\\b\\w\\b', ''),\n",
    "        (r'([a-zA-Z])\\1{2,}', r'\\1')\n",
    "    ]\n",
    "    \n",
    "    # Apply the replacements instructions\n",
    "    for key, value in replacements:\n",
    "        tweets[0] = tweets[0].str.replace(key, value)\n",
    "            \n",
    "    # Tokenize each tweet\n",
    "    tweets[0] = tweets[0].str.split()\n",
    "    \n",
    "    # Remove the stop words\n",
    "    tweets[0] = tweets[0].apply(lambda tweet: [word for word in tweet if word not in lang_set])\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = format_tweets(negative_DF)\n",
    "positive_DF = format_tweets(negative_DF)\n",
    "test_DF = format_tweets(negative_DF)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[vinco, tresorpack, difficulty, object, disass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[glad, dot, taks, tomorrow, thankful, startho]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[vs, celtics, regular, season, fucked, play, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[could, actually, kill, girl, sorry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[find, hard, believe, im, afraid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[wish, could, night, tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[got, kicked, wgm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[yes, tell, lips, closed, okay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[perfect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[hi, harry, havea, good, time, aus, didnt, get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  [vinco, tresorpack, difficulty, object, disass...\n",
       "1     [glad, dot, taks, tomorrow, thankful, startho]\n",
       "2  [vs, celtics, regular, season, fucked, play, p...\n",
       "3               [could, actually, kill, girl, sorry]\n",
       "4                  [find, hard, believe, im, afraid]\n",
       "5                      [wish, could, night, tonight]\n",
       "6                                 [got, kicked, wgm]\n",
       "7                    [yes, tell, lips, closed, okay]\n",
       "8                                          [perfect]\n",
       "9  [hi, harry, havea, good, time, aus, didnt, get..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Adrien
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai remarké que stemmetize n'est pas toujours la meilleure chose à faire et tente du coup aussi de lemmetize. Ca nous rajoute de ce faite deux colonnes, pour qu'on puisse ensuite sélectionner la meilleure à utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function applies stemming and lemmatizing to the given tweet set\n",
    "def stem_and_lem(tweets, stemmer, lemmer):\n",
    "    tweets['stemmed'] = tweets[0].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    tweets['lemmed'] = tweets[0].apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])\n",
    "    tweets['both'] = tweets['lemmed'].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating the stemmer and lemmatizer\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les lemmetizer ne semble pas marcher à 100% donc essayons de faire les deux (donc utiliser le stem sur le lemmetizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = stem_and_lem(negative_DF, stemmer, lemmatizer)\n",
    "positive_DF = stem_and_lem(positive_DF, stemmer, lemmatizer)\n",
    "test_DF = stem_and_lem(test_DF, stemmer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmed</th>\n",
       "      <th>both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[vinco, tresorpack, difficulty, object, disass...</td>\n",
       "      <td>[vinco, tresorpack, difficulti, object, disass...</td>\n",
       "      <td>[vinco, tresorpack, difficulty, object, disass...</td>\n",
       "      <td>[vinco, tresorpack, difficulti, object, disass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[glad, dot, taks, tomorrow, thankful, startho]</td>\n",
       "      <td>[glad, dot, tak, tomorrow, thank, startho]</td>\n",
       "      <td>[glad, dot, taks, tomorrow, thankful, startho]</td>\n",
       "      <td>[glad, dot, tak, tomorrow, thank, startho]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[vs, celtics, regular, season, fucked, play, p...</td>\n",
       "      <td>[vs, celtic, regular, season, fuck, play, play...</td>\n",
       "      <td>[v, celtic, regular, season, fucked, play, pla...</td>\n",
       "      <td>[v, celtic, regular, season, fuck, play, playoff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[could, actually, kill, girl, sorry]</td>\n",
       "      <td>[could, actual, kill, girl, sorri]</td>\n",
       "      <td>[could, actually, kill, girl, sorry]</td>\n",
       "      <td>[could, actual, kill, girl, sorri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[find, hard, believe, im, afraid]</td>\n",
       "      <td>[find, hard, believ, im, afraid]</td>\n",
       "      <td>[find, hard, believe, im, afraid]</td>\n",
       "      <td>[find, hard, believ, im, afraid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[wish, could, night, tonight]</td>\n",
       "      <td>[wish, could, night, tonight]</td>\n",
       "      <td>[wish, could, night, tonight]</td>\n",
       "      <td>[wish, could, night, tonight]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[got, kicked, wgm]</td>\n",
       "      <td>[got, kick, wgm]</td>\n",
       "      <td>[got, kicked, wgm]</td>\n",
       "      <td>[got, kick, wgm]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[yes, tell, lips, closed, okay]</td>\n",
       "      <td>[yes, tell, lip, close, okay]</td>\n",
       "      <td>[yes, tell, lip, closed, okay]</td>\n",
       "      <td>[yes, tell, lip, close, okay]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[perfect]</td>\n",
       "      <td>[perfect]</td>\n",
       "      <td>[perfect]</td>\n",
       "      <td>[perfect]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[hi, harry, havea, good, time, aus, didnt, get...</td>\n",
       "      <td>[hi, harri, havea, good, time, aus, didnt, get...</td>\n",
       "      <td>[hi, harry, havea, good, time, au, didnt, get,...</td>\n",
       "      <td>[hi, harri, havea, good, time, au, didnt, get,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  [vinco, tresorpack, difficulty, object, disass...   \n",
       "1     [glad, dot, taks, tomorrow, thankful, startho]   \n",
       "2  [vs, celtics, regular, season, fucked, play, p...   \n",
       "3               [could, actually, kill, girl, sorry]   \n",
       "4                  [find, hard, believe, im, afraid]   \n",
       "5                      [wish, could, night, tonight]   \n",
       "6                                 [got, kicked, wgm]   \n",
       "7                    [yes, tell, lips, closed, okay]   \n",
       "8                                          [perfect]   \n",
       "9  [hi, harry, havea, good, time, aus, didnt, get...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [vinco, tresorpack, difficulti, object, disass...   \n",
       "1         [glad, dot, tak, tomorrow, thank, startho]   \n",
       "2  [vs, celtic, regular, season, fuck, play, play...   \n",
       "3                 [could, actual, kill, girl, sorri]   \n",
       "4                   [find, hard, believ, im, afraid]   \n",
       "5                      [wish, could, night, tonight]   \n",
       "6                                   [got, kick, wgm]   \n",
       "7                      [yes, tell, lip, close, okay]   \n",
       "8                                          [perfect]   \n",
       "9  [hi, harri, havea, good, time, aus, didnt, get...   \n",
       "\n",
       "                                              lemmed  \\\n",
       "0  [vinco, tresorpack, difficulty, object, disass...   \n",
       "1     [glad, dot, taks, tomorrow, thankful, startho]   \n",
       "2  [v, celtic, regular, season, fucked, play, pla...   \n",
       "3               [could, actually, kill, girl, sorry]   \n",
       "4                  [find, hard, believe, im, afraid]   \n",
       "5                      [wish, could, night, tonight]   \n",
       "6                                 [got, kicked, wgm]   \n",
       "7                     [yes, tell, lip, closed, okay]   \n",
       "8                                          [perfect]   \n",
       "9  [hi, harry, havea, good, time, au, didnt, get,...   \n",
       "\n",
       "                                                both  \n",
       "0  [vinco, tresorpack, difficulti, object, disass...  \n",
       "1         [glad, dot, tak, tomorrow, thank, startho]  \n",
       "2  [v, celtic, regular, season, fuck, play, playoff]  \n",
       "3                 [could, actual, kill, girl, sorri]  \n",
       "4                   [find, hard, believ, im, afraid]  \n",
       "5                      [wish, could, night, tonight]  \n",
       "6                                   [got, kick, wgm]  \n",
       "7                      [yes, tell, lip, close, okay]  \n",
       "8                                          [perfect]  \n",
       "9  [hi, harri, havea, good, time, au, didnt, get,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
>>>>>>> Adrien
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function saves the processed tweets to a txt file\n",
    "def save_tweets(tweets, filename):\n",
    "    \n",
    "    # Put the stemmed and lemmetized tweets back to string form\n",
    "    data = tweets['lemmed'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"twitter-datasets/\" + filename, \"w\", encoding=\"utf8\") as myfile:\n",
    "        data.to_csv(myfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tweets(negative_DF, \"train_neg_proc.txt\")\n",
    "save_tweets(positive_DF, \"train_pos_proc.txt\")\n",
    "save_tweets(test_DF, \"test_data_proc.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il va falloir faire ce qui est proposé dans le pdf, donc par exemple, compter les mots qui apparaissent le plus dans negatif et positif, et pourquoi pas utiliser ça ensuite dans notre algorithme pour décider si c'est positif ou négatif dans le test. =)\n",
    "\n",
    "Quelques petites notes: \n",
    "- chaque tweet est un liste de mots\n",
    "- on a quatre colonnes: les mots de base du tweet, les mots mais stemmés (racine du mot), les mots mais lemmés (idem mais se veut plus précis) et un qui fait les deux (lem puis stem)\n",
    "- je pars du principe que tout est en anglais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings and index values\n",
    "emb = np.load(\"outputs/embeddings.npy\")\n",
    "\n",
    "vocab_cut = pd.read_csv(\"outputs/vocab_cut.txt\", sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "index = pd.Series(vocab_cut[vocab_cut.columns[0]].values)\n",
    "\n",
    "# Create word definition matrix\n",
    "word_weights = pd.DataFrame(data=emb, index=index)\n",
    "\n",
    "word_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This function removes words that are not in the vocabulary from the tweets\n",
    "def clean_tweets(tweets, vocab):\n",
    "    clean = tweets.copy().apply(lambda tweet: [word for word in tweet if word in vocab.values])\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract just the column we need\n",
    "neg_DF = negative_DF[\"lemmed\"]\n",
    "pos_DF = positive_DF[\"lemmed\"]\n",
    "test_DF = test_DF[\"lemmed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Unused</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We want a sparse matrix that tells us for each tweet (rows) how many times a given\n",
    "# word (columns) appears (\"bags of words\" representation)\n",
    "\n",
    "# MIGHT NEED TO BE REDEFINED WITH neg_words AND pos_words INSTEAD !!!!!!\n",
    "def bags_of_words(tweets=None):\n",
    "    \n",
    "    # We get the vocabulary\n",
    "    with open('outputs/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "        \n",
    "    col = 0\n",
    "    data, rows, cols = [], [], []\n",
    "    \n",
    "    for tweet in tweets:\n",
    "        # We count word occurences in a tweet\n",
    "        word_count = Counter(tweet).items()\n",
    "        \n",
    "        # If the word is in the vocabulary we add it in the matrix\n",
    "        for word, count in word_count:\n",
    "            row = vocab.get(word)\n",
    "            \n",
    "            if row:\n",
    "                data.append(count)\n",
    "                cols.append(col)\n",
    "                rows.append(row)\n",
    "        \n",
    "        col += 1\n",
    "        \n",
    "    # We convert the scipy.sparse matrix to pandas.SparseSeries for ease of use\n",
    "    return pd.SparseSeries.from_coo(sparse.coo_matrix((data, (rows, cols))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bags = bags_of_words(pos_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Random Forest Classifier</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(path_glove_twitter, word_index, nb_words, embedding_dim):\n",
    "    \n",
    "    # create index mapping words in the embeddings to their embedding vector\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    f = open(path_glove_twitter, \"r\", encoding=\"utf-8\") \n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        # for each word we find the corresponding word vector\n",
    "        embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    # Create the embeding matrix corresponding to our Dataset\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if i > nb_words: \n",
    "            continue\n",
    "            \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=nb_word)\n",
    "tokenizer.fit_on_texts(train)\n",
    "sequences_train = tokenizer.texts_to_sequences(train)\n",
    "sequences_test = tokenizer.texts_to_sequences(test)\n",
    "# take only the index of words\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = embedding_matrix(\"outputs/embeddings\", word_index, len(word_index), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sums the embeddings of each word in the given tweet\n",
    "\n",
    "# THIS METHOD IS NOT OPTIMAL AND WOULD BENEFIT BEING MADE FASTER UNLESS WE DO NOT USE IT IN THE END\n",
    "def query_weights(tweet):\n",
    "    w = pd.DataFrame(columns=range(20))\n",
    "    \n",
    "    for word in tweet:\n",
    "        try:\n",
    "            w = w.append(word_weights.loc[word, :])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return w.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tweet embeddings\n",
    "neg_dims = neg_DF.copy().apply(query_weights)\n",
    "pos_dims = pos_DF.copy().apply(query_weights)\n",
    "test_dims = test_DF.copy().apply(query_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the embeddings in pkl files\n",
    "with open('outputs/neg_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_dims, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('outputs/pos_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_dims, f, pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "with open('outputs/test_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dims, f, pickle.HIGHEST_PROTOCOL)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the embeddings from pkl files\n",
    "with open('outputs/neg_dims.pkl', 'rb') as f:\n",
    "    neg_dims = pickle.load(f)\n",
    "    \n",
    "with open('outputs/pos_dims.pkl', 'rb') as f:\n",
    "    pos_dims = pickle.load(f)\n",
    "    \n",
    "with open('outputs/test_dims.pkl', 'rb') as f:\n",
    "    test_dims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the matrices for SVM fitting, we just put the positive and negative embeddings together and\n",
    "# create the appropriate y matrix with 1's and -1's\n",
    "X = pos_dims.append(neg_dims)\n",
    "ones = np.ones((pos_dims.shape[0], 1))\n",
    "y = np.append(ones, -1 * ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applies the Random Forest Classifier technique to the data\n",
    "start = time.time()\n",
    "clf = RandomForestClassifier(min_samples_leaf=20)\n",
    "clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Random Forest\", end - start, clf.score(X, y))\n",
    "pred = pd.DataFrame(clf.predict(test_dims))\n",
    "pred.columns = [\"Prediction\"]\n",
    "pred.insert(0, \"Id\", pred.index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We save the submission\n",
    "pred.to_csv(\"outputs/submission.csv\", index=False, float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply cross validation to the data\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorizer. We go with the idea that we do not want the words that appear in less than 5 tweets and in more than 80% of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vectoriser\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create a corpus. Our train set would both positive and negative, and our test set is, obviously, the unlabeled part.\n",
    "\n",
    "To do this, we will append both negative and positive DF, then create a matrix of labels for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg_DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-dcc19a48ef92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# put the list of words into a usable format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mneg_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpos_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mneg_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneg_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpos_DF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"both\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_DF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'neg_DF' is not defined"
     ]
    }
   ],
   "source": [
    "# put the list of words into a usable format\n",
    "neg_DF = pd.DataFrame(neg_DF)\n",
    "pos_DF = pd.DataFrame(pos_DF)\n",
    "test_DF = pd.DataFrame(test_DF)\n",
    "neg_DF[\"lemmed\"] = neg_DF.lemmed.apply(' '.join)\n",
    "pos_DF[\"lemmed\"] = pos_DF.lemmed.apply(' '.join)\n",
    "test_DF[\"lemmed\"] = test_DF.lemmed.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we thus know that all the first ones are labeled as -1 and all the others as 1\n",
    "all_labeled_DF = pd.concat([neg_DF, pos_DF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create the labels\n",
    "negs = len(neg_DF.index)\n",
    "poss = len(pos_DF.index)\n",
    "labels = np.zeros(negs+poss)\n",
    "labels[0:negs]=-1\n",
    "labels[negs:negs+poss]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_corpus_tf_idf = vectorizer.fit_transform(all_labeled_DF[\"lemmed\"]) \n",
    "test_corpus_tf_idf = vectorizer.transform(test_DF[\"lemmed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create both models\n",
    "model1 = LinearSVC() # SVM\n",
    "model2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on the given models\n",
    "model1.fit(train_corpus_tf_idf,labels)\n",
    "model2.fit(train_corpus_tf_idf,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "result1 = model1.predict(test_corpus_tf_idf)\n",
    "result2 = model2.predict(test_corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result1 and result2 are the labels predicted for the tweets we got in the test corpus. This means we probably jsute have to transforme this into a csv as it is shown in the sample submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting it to integer for prediction csv\n",
    "result1 = [int(x) for x in result1]\n",
    "result2 = [int(x) for x in result2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_df = pd.DataFrame(result1)\n",
    "svm_df['Id'] = svm_df.index + 1\n",
    "svm_df['Prediction'] = svm_df[0]\n",
    "svm_df = svm_df[['Id', 'Prediction']]\n",
    "svm_df.to_csv('svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_df = pd.DataFrame(result2)\n",
    "bayes_df['Id'] = bayes_df.index + 1\n",
    "bayes_df['Prediction'] = bayes_df[0]\n",
    "bayes_df = bayes_df[['Id', 'Prediction']]\n",
    "bayes_df.to_csv('bayes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
