{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest implementation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, nltk, pickle, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.25816056, -0.29469185, -1.06713131, ...,  0.36700308,\n",
       "        -0.14723212,  0.09652655],\n",
       "       [-0.17024545, -0.05926867, -0.41807964, ...,  0.23533832,\n",
       "        -0.15981563,  0.09256065],\n",
       "       [-0.15801784, -0.06317051, -0.42592788, ...,  0.24630222,\n",
       "        -0.2073824 ,  0.13902852],\n",
       "       ..., \n",
       "       [ 0.9867201 ,  1.99722096, -0.09613431, ..., -0.19128622,\n",
       "         0.28879319, -0.39757371],\n",
       "       [-0.53913936,  0.43205019, -1.05278255, ..., -0.62491689,\n",
       "        -1.3973401 , -0.26574516],\n",
       "       [-0.14142788,  0.44970575,  0.61911085, ..., -1.57691884,\n",
       "         0.362506  , -0.90781572]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "vocab_cut = pd.read_csv(\"outputs/vocab_cut.txt\", sep=\" \", header=None, quoting=csv.QUOTE_NONE)\n",
    "index = pd.Series(vocab_cut[vocab_cut.columns[0]].values)\n",
    "emb = np.load(\"outputs/embeddings.npy\")\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(emb)):\n",
    "    values = emb[i]\n",
    "    word = index[i]\n",
    "    # for each word we find the corresponding word vector\n",
    "    embeddings_index[word] = np.asarray(values[:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_matrix(path_glove_twitter, word_index, nb_words, embedding_dim):\n",
    "    \n",
    "    # create index mapping words in the embeddings to their embedding vector\n",
    "    embeddings_index = {}\n",
    "    \n",
    "    f = open(path_glove_twitter, \"r\", encoding=\"utf-8\") \n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        # for each word we find the corresponding word vector\n",
    "        embeddings_index[word] = np.asarray(values[1:], dtype='float32')\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "    # Create the embeding matrix corresponding to our Dataset\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embedding_dim))\n",
    "    \n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if i > nb_words: \n",
    "            continue\n",
    "            \n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=nb_word)\n",
    "tokenizer.fit_on_texts(train)\n",
    "sequences_train = tokenizer.texts_to_sequences(train)\n",
    "sequences_test = tokenizer.texts_to_sequences(test)\n",
    "# take only the index of words\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = embedding_matrix(\"outputs/embeddings\", word_index, len(word_index), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sums the embeddings of each word in the given tweet\n",
    "\n",
    "# THIS METHOD IS NOT OPTIMAL AND WOULD BENEFIT BEING MADE FASTER UNLESS WE DO NOT USE IT IN THE END\n",
    "def query_weights(tweet):\n",
    "    w = pd.DataFrame(columns=range(20))\n",
    "    \n",
    "    for word in tweet:\n",
    "        try:\n",
    "            w = w.append(word_weights.loc[word, :])\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "    return w.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tweet embeddings\n",
    "neg_dims = neg_DF.copy().apply(query_weights)\n",
    "pos_dims = pos_DF.copy().apply(query_weights)\n",
    "test_dims = test_DF.copy().apply(query_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the embeddings in pkl files\n",
    "with open('outputs/neg_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(neg_dims, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('outputs/pos_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(pos_dims, f, pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "with open('outputs/test_dims.pkl', 'wb') as f:\n",
    "    pickle.dump(test_dims, f, pickle.HIGHEST_PROTOCOL)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the embeddings from pkl files\n",
    "with open('outputs/neg_dims.pkl', 'rb') as f:\n",
    "    neg_dims = pickle.load(f)\n",
    "    \n",
    "with open('outputs/pos_dims.pkl', 'rb') as f:\n",
    "    pos_dims = pickle.load(f)\n",
    "    \n",
    "with open('outputs/test_dims.pkl', 'rb') as f:\n",
    "    test_dims = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate the matrices for SVM fitting, we just put the positive and negative embeddings together and\n",
    "# create the appropriate y matrix with 1's and -1's\n",
    "X = pos_dims.append(neg_dims)\n",
    "ones = np.ones((pos_dims.shape[0], 1))\n",
    "y = np.append(ones, -1 * ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applies the Random Forest Classifier technique to the data\n",
    "start = time.time()\n",
    "clf = RandomForestClassifier(min_samples_leaf=20)\n",
    "clf.fit(X, y)\n",
    "end = time.time()\n",
    "print(\"Random Forest\", end - start, clf.score(X, y))\n",
    "pred = pd.DataFrame(clf.predict(test_dims))\n",
    "pred.columns = [\"Prediction\"]\n",
    "pred.insert(0, \"Id\", pred.index + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We save the submission\n",
    "pred.to_csv(\"outputs/sub_random_forest.csv\", index=False, float_format=\"%.0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Apply cross validation to the data\n",
    "scores = cross_val_score(clf, X, y, cv=10)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
