{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Natural language processing library.\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# need to use once to download nltk on your computer.\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vu qu'on a une librarie qui nous permet de faire pas mal de choses, on va:\n",
    "- mettre tout en minuscule\n",
    "- retirer la ponctuation\n",
    "- retirer tous les nombres et caractères non alphanumériques\n",
    "- les tokeniser: donc séparer les mots\n",
    "- retirer les stop words (and, the, etc)\n",
    "- stemmer -> avoir que la racine de chaque mots\n",
    "\n",
    "à partir de là on aura déjà un dataset plus ou moins propre =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create empty arrays wich will get textfile data\n",
    "data_neg = []\n",
    "data_pos = []\n",
    "data_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each new line will be a new element of array\n",
    "with open (\"twitter-datasets/train_neg_full.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    data_neg = myfile.readlines()\n",
    "with open (\"twitter-datasets/train_pos_full.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    data_pos = myfile.readlines()\n",
    "with open (\"twitter-datasets/test_data.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    data_test = myfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform array into Dataframe to use it easily\n",
    "negative_DF = pd.DataFrame(data_neg)\n",
    "positive_DF = pd.DataFrame(data_pos)\n",
    "test_DF = pd.DataFrame(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Put everything to lowercase.\n",
    "negative_DF[0] = negative_DF[0].astype(str).str.lower()\n",
    "positive_DF[0] = positive_DF[0].astype(str).str.lower()\n",
    "test_DF[0] = test_DF[0].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take out user and url\n",
    "negative_DF[0] = negative_DF[0].str.replace(\"<user>\", '')\n",
    "negative_DF[0] = negative_DF[0].str.replace(\"<url>\", '')\n",
    "\n",
    "positive_DF[0] = positive_DF[0].str.replace(\"<user>\", '')\n",
    "positive_DF[0] = positive_DF[0].str.replace(\"<url>\", '')\n",
    "\n",
    "test_DF[0] = test_DF[0].str.replace(\"<user>\", '')\n",
    "test_DF[0] = test_DF[0].str.replace(\"<url>\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#take out punctuation and non alphanumerical characters\n",
    "negative_DF[0] = negative_DF[0].str.replace(r'[^\\w\\s]', '')\n",
    "positive_DF[0] = positive_DF[0].str.replace(r'[^\\w\\s]', '')\n",
    "test_DF[0] = test_DF[0].str.replace(r'[^\\w\\s]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take out numbers\n",
    "negative_DF[0] = negative_DF[0].str.replace(r'[\\d]', '')\n",
    "positive_DF[0] = positive_DF[0].str.replace(r'[\\d]', '')\n",
    "test_DF[0] = test_DF[0].str.replace(r'[\\d]', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize each one.\n",
    "negative_DF[0] = negative_DF[0].str.split()\n",
    "positive_DF[0] = positive_DF[0].str.split()\n",
    "test_DF[0] = test_DF[0].str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we remove the stop words\n",
    "lang_set = nltk.corpus.stopwords.words('english')\n",
    "negative_DF[0] = negative_DF[0].apply(lambda x: [word for word in x if word not in lang_set])\n",
    "positive_DF[0] = positive_DF[0].apply(lambda x: [word for word in x if word not in lang_set])\n",
    "test_DF[0] = test_DF[0].apply(lambda x: [word for word in x if word not in lang_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we remove the rt (retweets)\n",
    "negative_DF[0] = negative_DF[0].apply(lambda x: [word for word in x if word not in ['rt']])\n",
    "positive_DF[0] = positive_DF[0].apply(lambda x: [word for word in x if word not in ['rt']])\n",
    "test_DF[0] = test_DF[0].apply(lambda x: [word for word in x if word not in ['rt']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'ai remarké que stemmetize n'est pas toujours la meilleure chose à faire et tente du coup aussi de lemmetize. Ca nous rajoute de ce faite deux colonnes, pour qu'on puisse ensuite sélectionner la meilleure à utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we stemmatize (get the root only) for each word\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')    \n",
    "negative_DF['stemmed'] = negative_DF[0].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "positive_DF['stemmed'] = positive_DF[0].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_DF['stemmed'] = test_DF[0].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "negative_DF['lemmed'] = negative_DF[0].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "positive_DF['lemmed'] = positive_DF[0].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n",
    "test_DF['lemmed'] = test_DF[0].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les lemmetizer ne semble pas amrcher à 100% donc essayons de faire les deux (donc utiliser le stem sur le lemmetizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF['both'] = negative_DF['lemmed'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "positive_DF['both'] = positive_DF['lemmed'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "test_DF['both'] = test_DF['lemmed'].apply(lambda x: [stemmer.stem(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, il va falloir faire ce qui est proposé dans le pdf, donc par exemple, compter les mots qui apparaissent le plus dans negatif et positif, et pourquoi pas utiliser ça ensuite dans notre algorithme pour décider si c'est positif ou négatif dans le test. =)\n",
    "\n",
    "Quelques petites notes: \n",
    "- chaque tweet est un liste de mots\n",
    "- on a trois colonnes: les mots de base du tweet, les mots mais stemmés (racine du mot), les motsi mais lemmés (idem mais se veut plus précis)\n",
    "- je pars du principe que tout est en anglais."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
