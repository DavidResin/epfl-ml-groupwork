{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv, nltk, pickle, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"twitter-datasets/train_neg_proc.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    neg_DF = pd.read_csv(myfile, header=None)\n",
    "with open(\"twitter-datasets/train_pos_proc.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    pos_DF = pd.read_csv(myfile, header=None)\n",
    "with open(\"twitter-datasets/test_data_proc.txt\", \"r\", encoding=\"utf8\") as myfile:\n",
    "    test_DF = pd.read_csv(myfile, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>TF-IDF</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the vectorizer. We go with the idea that we do not want the words that appear in less than 5 tweets and in more than 80% of the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the vectoriser\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df = 0.8, sublinear_tf=True, use_idf =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to create a corpus. Our train set would both positive and negative, and our test set is, obviously, the unlabeled part.\n",
    "\n",
    "To do this, we will append both negative and positive DF, then create a matrix of labels for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we thus know that all the first ones are labeled as -1 and all the others as 1\n",
    "all_labeled_DF = pd.concat([neg_DF, pos_DF])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we create the labels\n",
    "negs = len(neg_DF.index)\n",
    "poss = len(pos_DF.index)\n",
    "labels = np.zeros(negs+poss)\n",
    "labels[0:negs]=-1\n",
    "labels[negs:negs+poss]=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus_tf_idf = vectorizer.fit_transform(all_labeled_DF) \n",
    "test_corpus_tf_idf = vectorizer.transform(test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create both models\n",
    "model1 = LinearSVC() # SVM\n",
    "model2 = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train on the given models\n",
    "model1.fit(train_corpus_tf_idf,labels)\n",
    "model2.fit(train_corpus_tf_idf,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions\n",
    "result1 = model1.predict(test_corpus_tf_idf)\n",
    "result2 = model2.predict(test_corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result1 and result2 are the labels predicted for the tweets we got in the test corpus. This means we probably jsute have to transforme this into a csv as it is shown in the sample submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting it to integer for prediction csv\n",
    "result1 = [int(x) for x in result1]\n",
    "result2 = [int(x) for x in result2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_df = pd.DataFrame(result1)\n",
    "svm_df['Id'] = svm_df.index + 1\n",
    "svm_df['Prediction'] = svm_df[0]\n",
    "svm_df = svm_df[['Id', 'Prediction']]\n",
    "svm_df.to_csv('svm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_df = pd.DataFrame(result2)\n",
    "bayes_df['Id'] = bayes_df.index + 1\n",
    "bayes_df['Prediction'] = bayes_df[0]\n",
    "bayes_df = bayes_df[['Id', 'Prediction']]\n",
    "bayes_df.to_csv('bayes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
