{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Cleaning</h1>\n",
    "\n",
    "In this notebook we clean the provided datasets by usding natural language processing methods on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Needed imports\n",
    "import nltk, pickle, re\n",
    "import pandas as pd\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet extraction</h3>\n",
    "\n",
    "This function extracts tweet data from a given filename and puts it in a dataframe. We then apply it to labeled tweets datasets (to get positives and negatives) and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets(filename):\n",
    "    \n",
    "    # Read the data file\n",
    "    with open(\"twitter-datasets/\" + filename, \"r\", encoding=\"utf8\") as myfile:\n",
    "        data = myfile.readlines()\n",
    "        \n",
    "    # Make a dataframe out of the data\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = get_tweets(\"train_neg_full.txt\")\n",
    "positive_DF = get_tweets(\"train_pos_full.txt\")\n",
    "test_DF = get_tweets(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet Formatting</h3>\n",
    "\n",
    "In this function, we format the data to suit our needs. As such, we put everything to lowercase, remove unwanted elements such as usertags, urls, retweet mentions (rt) and lone characters. We also replace non-alphabetical characters by spaces and sequences of 3 times or more of the same character by a single occurence of said character.\n",
    "\n",
    "We also remove the (English) stop words found in the the nltk package (words such as \"I\", \"you\", etc...). Finally, the strings are replaced by lists of words. This function is then applied to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_tweets(tweets):\n",
    "    \n",
    "    # These are stop words that we want to take out from the tweets\n",
    "    lang_set = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Put everything in lowercase\n",
    "    tweets[0] = tweets[0].astype(str).str.lower()\n",
    "    \n",
    "    # The replacement instructions for below, which:\n",
    "    # - remove usertags\n",
    "    # - remove urls\n",
    "    # - remove retweets (\"rt\")\n",
    "    # - replace anything that is not letters by a space\n",
    "    # - remove lone characters\n",
    "    # - replace sequences of 3 times the same letter or more by a single occurence of the character\n",
    "    replacements = [\n",
    "        (\"<user>\", ''),\n",
    "        (\"<url>\", ''),\n",
    "        (r'\\brt\\b', ''),\n",
    "        (r'[^a-z]+', ' '),\n",
    "        (r'\\b\\w\\b', ''),\n",
    "        (r'([a-zA-Z])\\1{2,}', r'\\1')\n",
    "    ]\n",
    "    \n",
    "    # Apply the replacements instructions\n",
    "    for key, value in replacements:\n",
    "        tweets[0] = tweets[0].str.replace(key, value)\n",
    "            \n",
    "    # Tokenize each tweet\n",
    "    tweets[0] = tweets[0].str.split()\n",
    "    \n",
    "    # Remove the stop words\n",
    "    tweets[0] = tweets[0].apply(lambda tweet: [word for word in tweet if word not in lang_set])\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = format_tweets(negative_DF)\n",
    "positive_DF = format_tweets(positive_DF)\n",
    "test_DF = format_tweets(test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming and Lemmatizing</h3>\n",
    "\n",
    "Now that our tweets are cleaned, we apply stemming (crude chopping of the end of the words) and lemming (process that actually looks into a vocabulary to try to get the roots of words) to each word in the list to make words such as \"love\" and \"lover\" to be recognized as the same in order to increase efficiency down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_and_lem(tweets, stemmer, lemmer):\n",
    "    tweets['stemmed'] = tweets[0].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    tweets['lemmed'] = tweets[0].apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])\n",
    "    tweets['both'] = tweets['lemmed'].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating the stemmer and lemmatizer\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = stem_and_lem(negative_DF, stemmer, lemmatizer)\n",
    "positive_DF = stem_and_lem(positive_DF, stemmer, lemmatizer)\n",
    "test_DF = stem_and_lem(test_DF, stemmer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save data</h3>\n",
    "\n",
    "All we have to do now is save our results in txt files so that they can be used in our different methods. We also save the DataFrames in pickle files to use them instead for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tweets(tweets, filename):\n",
    "    \n",
    "    # Put the stemmed and lemmetized tweets back to string form\n",
    "    data = tweets['lemmed'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"twitter-datasets/\" + filename, \"w\", encoding=\"utf8\") as myfile:\n",
    "        data.to_csv(myfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tweets(negative_DF, \"train_neg_proc.txt\")\n",
    "save_tweets(positive_DF, \"train_pos_proc.txt\")\n",
    "save_tweets(test_DF, \"test_data_proc.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('outputs/train_neg_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(negative_DF, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('outputs/train_pos_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(positive_DF, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('outputs/test_data_proc.pkl', 'wb') as f:\n",
    "    pickle.dump(test_DF, f, pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
