{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Processing of tweets</h1>\n",
    "\n",
    "In this notebook we process the provided datasets to improve the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re\n",
    "import pandas as pd\n",
    "\n",
    "# need to use once to download nltk (natural language processing library) on your computer.\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet extraction</h3>\n",
    "\n",
    "This function extracts tweet data from a given filename and puts it in a dataframe. We then apply it to the full tweet datasets and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tweets(filename):\n",
    "    \n",
    "    # Read the data file\n",
    "    with open(\"twitter-datasets/\" + filename, \"r\", encoding=\"utf8\") as myfile:\n",
    "        data = myfile.readlines()\n",
    "        \n",
    "    # Make a dataframe out of the data\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = get_tweets(\"train_neg_full.txt\")\n",
    "positive_DF = get_tweets(\"train_pos_full.txt\")\n",
    "test_DF = get_tweets(\"test_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tweet formatting</h3>\n",
    "\n",
    "This function does the job of formatting tweet data to suit our needs. It puts everything to lowercase, removes unwanted elements such as usertags, urls, retweet tags and lone characters and replaces non-alphabetical characters by spaces and sequences of 3 times the same character or more by a single occurence of the character. We also remove the english language stopwords of the nltk package (words such as \"I\", \"you\", etc...). Finally, the strings are replaced by lists of words. The function is then applied to each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_tweets(tweets):\n",
    "    \n",
    "    # These are stop words that we want to take out from the tweets\n",
    "    lang_set = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # Put everything in lowercase\n",
    "    tweets[0] = tweets[0].astype(str).str.lower()\n",
    "    \n",
    "    # The replacement instructions for below, which:\n",
    "    # - remove usertags\n",
    "    # - remove urls\n",
    "    # - remove retweets (\"rt\")\n",
    "    # - replace anything that is not letters by a space\n",
    "    # - remove lone characters\n",
    "    # - replace sequences of 3 times the same letter or more by a single occurence of the character\n",
    "    replacements = [\n",
    "        (\"<user>\", ''),\n",
    "        (\"<url>\", ''),\n",
    "        (r'\\brt\\b', ''),\n",
    "        (r'[^a-z]+', ' '),\n",
    "        (r'\\b\\w\\b', ''),\n",
    "        (r'([a-zA-Z])\\1{2,}', r'\\1')\n",
    "    ]\n",
    "    \n",
    "    # Apply the replacements instructions\n",
    "    for key, value in replacements:\n",
    "        tweets[0] = tweets[0].str.replace(key, value)\n",
    "            \n",
    "    # Tokenize each tweet\n",
    "    tweets[0] = tweets[0].str.split()\n",
    "    \n",
    "    # Remove the stop words\n",
    "    tweets[0] = tweets[0].apply(lambda tweet: [word for word in tweet if word not in lang_set])\n",
    "        \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = format_tweets(negative_DF)\n",
    "positive_DF = format_tweets(positive_DF)\n",
    "test_DF = format_tweets(test_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Stemming and lemmatizing</h3>\n",
    "\n",
    "Now that our tweets are cleaned, we apply stemming (crude chopping of the end of the words) and lemming (process that actually looks into vocabulary to try to simplify words) to the words to make words such as \"love\" and \"lover\" to be recognized as the same in order to increase efficiency down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem_and_lem(tweets, stemmer, lemmer):\n",
    "    tweets['stemmed'] = tweets[0].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    tweets['lemmed'] = tweets[0].apply(lambda tweet: [lemmatizer.lemmatize(word) for word in tweet])\n",
    "    tweets['both'] = tweets['lemmed'].apply(lambda tweet: [stemmer.stem(word) for word in tweet])\n",
    "    \n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generating the stemmer and lemmatizer\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF = stem_and_lem(negative_DF, stemmer, lemmatizer)\n",
    "positive_DF = stem_and_lem(positive_DF, stemmer, lemmatizer)\n",
    "test_DF = stem_and_lem(test_DF, stemmer, lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_DF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Save data</h3>\n",
    "\n",
    "All we have to do now is save our results in txt files so that they can be used in our different methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_tweets(tweets, filename):\n",
    "    \n",
    "    # Put the stemmed and lemmetized tweets back to string form\n",
    "    data = tweets['lemmed'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Save to file\n",
    "    with open(\"twitter-datasets/\" + filename, \"w\", encoding=\"utf8\") as myfile:\n",
    "        data.to_csv(myfile, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_tweets(negative_DF, \"train_neg_proc.txt\")\n",
    "save_tweets(positive_DF, \"train_pos_proc.txt\")\n",
    "save_tweets(test_DF, \"test_data_proc.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
